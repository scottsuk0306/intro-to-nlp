# 주영_Seq2Seq

## 15.1 Sequence-to-Sequence

Sequence-to-Seqeunce란 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델이다. 그 예로는 챗봇(Chatbot)과 기계 번역(Machine Translation)이 있다.

![seq2seq모델11.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_Seq2Seq%20dae29f0905d24530849e069da3c1778d/seq2seq%EB%AA%A8%EB%8D%B811.png)

Seq2seq 모델은 인코더와 디코더로 구성된다. 인코더는 모든 단어를 입력받은 후 벡터로 만드는데, 이를 컨텍스트 벡터 (Context vector)이라고 한다. 디코더는 컨텍스트 벡터를 받아 번역된 단어를 순차적으로 출력한다.

![인코더디코더모델.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_Seq2Seq%20dae29f0905d24530849e069da3c1778d/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.png)

인코더와 디코더를 더 자세히 파헤쳐 보자. 인코더는 입력 문장을 받는 RNN 셀이다. (실제로는 LSTM 또는 GRU 셀을 사용한다.) 

인코더에서는 문장이 단어 단위로 쪼개지고 각 단어가 RNN 셀 각 시점의 입력이 된다. 이 RNN 셀의 마지막 시점의 은닉 상태를 컨텍스트 벡터라고 한다.

디코더는 문장의 시작을 의미하는 심볼이 들어오면 다음에 등장할 확률이 높은 단어를 예측한다. 예측된 단어는 다음 시점의 RNN 셀의 입력이 된다.

![decodernextwordprediction.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_Seq2Seq%20dae29f0905d24530849e069da3c1778d/decodernextwordprediction.png)

출력 단어로 될 수 있는 단어는 다양하므로 소프트맥스 함수를 통해 출력 벡터를 각 단어별 확률 값으로 바꾼다.

### 15.1.2 글자 레벨 기계 번역기(Character-Level Neural Machine Translation) 구현하기

[Google Colaboratory](https://colab.research.google.com/drive/1lC52wBXvvkdH57rxTWoD2H2T5MnaTl2S?usp=sharing)

여기에서 교사 강요(Teacher Forcing)이 사용된다. 훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않는데, 이는 초기에 예측 오류가 연쇄적으로 작용하여 디코더 전체의 예측을 어렵게 할 수 있기 때문이다. RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 한다.

## 15.2 Neural Machine Translation (seq2seq) Tutorial

[Google Colaboratory](https://colab.research.google.com/drive/1bnISolciTn8gBwCxcfj4bpUUj14yHfYV?usp=sharing)

## 15.3 BLEU Score(Bilingual Evaluation Understudy Score)

BLEU Score이란 seq2seq 문제의 성능을 판단하는 지표이다.