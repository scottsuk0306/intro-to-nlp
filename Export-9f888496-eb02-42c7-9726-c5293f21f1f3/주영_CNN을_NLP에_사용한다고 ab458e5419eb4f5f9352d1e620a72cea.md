# 주영_CNN을_NLP에_사용한다고?

### 1. 합성곱 신경망 (Convolution Neural Network)

위키 닥스 참고해주세요.

### 2. 자연어 처리를 위한 1D CNN (1D Convolutional Neural Networks)

### 2.1 1D 합성곱 (1D Convolutions)

이미지 처리에서 사용하는 2D CNN은 익숙하지만, 자연어 처리에서 1D CNN은 다소 생소하다. 어떤 식으로 사용되는지 알아보자.

RNN, LSTM, GRU 등을 이용해 코드를 짤 때, 각 문장은 임베딩 층을 지나서 단어가 임베딩 벡터가 된 상태로 input이 되었다. 비슷한 방식으로 1D CNN 또한 벡터로 변환된 문장 행렬을 input으로 받는다. 

 

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/sentence_matrix.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/sentence_matrix.png)

wait for the video and don't rent it 문장 행렬

위의 사진은 문장이 k*n의 행렬로 변환된 것을 보여준다. 이 때, n은 문장의 길이, k는 임베딩 벡터의 차원을 말한다.

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/1d_cnn.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/1d_cnn.png)

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/%EB%84%A4%EB%B2%88%EC%A7%B8%EC%8A%A4%ED%85%9D.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/%EB%84%A4%EB%B2%88%EC%A7%B8%EC%8A%A4%ED%85%9D.png)

1D CNN의 커널의 너비는 문장 행렬의 임베딩 벡터의 차원과 동일하게 설정된다. 이렇게 되면 커널은 횡으로는 움직일 수가 없게 되고, 결과적으로는 8차원의 벡터가 나오게 된다. 2D CNN과 동일하게, 커널의 사이즈는 자유자재로 조절이 가능하다.

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/%EC%BB%A4%EB%84%903.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/%EC%BB%A4%EB%84%903.png)

위의 예시는 커널의 사이즈가 3인 경우이다. 이렇게 되면 나오게 되는 벡터의 차원은 7차원이 된다. 그렇다면 커널의 사이즈는 어떤 의미가 있을까? 커널의 사이즈는 자연어 처리 관점에서 참고하는 단어의 묶음을 의미한다. 커널의 사이즈를 바꾼다는 것은 참고하는 n-gram이 달라진다고 볼 수 있다.

### 2.2 맥스 풀링 (Max-pooling)

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/%EB%A7%A5%EC%8A%A4%ED%92%80%EB%A7%81.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/%EB%A7%A5%EC%8A%A4%ED%92%80%EB%A7%81.png)

일반적으로 CNN에서는 합성곱 층 다음에는 풀링 층을 추가하게 된다. 풀링 층에서는 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄이는 풀링 연산이 이루어진다. 여기서는 많이 사용되는 최대  풀링(max-pooling)을 사용하여 벡터에서 가장 큰 값을 꺼내온다.

### 2.3 신경망 설계하기

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/conv1d.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/conv1d.png)

위 신경망은 이진 분류를 위한 신경망이다. 문장 행렬은 사이즈 4인 커널 2개, 3인 커널 2개, 2인 커널 2개, 총 커널 6개에 의해 벡터로 변환된다. 각 벡터는 풀링 층에 의해서 6개의 스칼라 값이 되고, 스칼라 값은 연결(concatenate)되어 하나의 벡터가 된 후, 뉴런이 2개인 출력층에 연결되어 텍스트 분류를 수행하게 된다.

출력층이 2개인 자연어 처리 문제는 대표적으로 스팸 메일 분류가 있다. 신경망을 간단히 살펴보자. 

```python
model = Sequential()
model.add(Embedding(vocab_size, 32))
model.add(Dropout(0.2))
model.add(Conv1D(32, 5, strides=1, padding='valid', activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.summary()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
```

### 3. 글자 임베딩 (Character Embedding)

챕터 10에서 ELMo를 다룰 때, 잠깐 Character embedding에 대해서 언급이 나왔었는데, 여기서 더 자세한 내용을 다뤄보도록 하겠다. Charactrer embedding을 사용하게 되면 FastText의 subword와 같이 OOV (Out-of-Vocabulary)에 견고하다는 장점이 있다. 

### 3.1 1D CNN을 이용한 글자 임베딩

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/%EC%BA%A1%EC%B2%981.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_CNN%E1%84%8B%E1%85%B3%E1%86%AF_NLP%E1%84%8B%E1%85%A6_%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%83%E1%85%A1%E1%84%80%E1%85%A9%20ab458e5419eb4f5f9352d1e620a72cea/%EC%BA%A1%EC%B2%981.png)

1D CNN을 이용한 텍스트 분류와 매우 유사하다. 다른 점은 행렬이 문장 행렬에서 글자 행렬로 바뀐 것 뿐이다. 

### 3.2 BiLSTM을 이용한 글자 임베딩