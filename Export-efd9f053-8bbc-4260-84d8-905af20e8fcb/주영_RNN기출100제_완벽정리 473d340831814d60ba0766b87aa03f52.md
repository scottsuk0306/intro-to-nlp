# 주영_RNN기출100제_완벽정리

RNN은 앞선 8장에서 배운 Multi-Layer Perceptron (이하 MLP)와 조금 다르다. 설명에 따르면 Sequence, 즉 순서를 가진 데이터를 다룰 때 사용한다고 하는데, 우리가 기존에 알던 MLP와 어떤 점에서 다른지, 어떤 문제를 해결하려고 고안되었는지 알아보자. 또, 더 나아가서 RNN에서 파생된 LSTM, GRU 또한 알아볼 것이다.

### Sequence Model은 뭐지?

RNN은 대표적인 Sequence Model이다. 그럼 Sequence Model은 뭘까? 

Sequence Model이란 입력 혹은 출력이 시퀀스(sequence)인 것을 말한다. Sequence model이 사용되는 분야는 다음과 같은 것들이 있다.

1. 음성 인식 (Speech Recognition)
2. 텍스트 내 감정 분석 (Sentiment Analysis)
3. 챗봇, 인공지능 비서 개발
4. 영상 내 동작 인식 (Video Activity Recognition)

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_RNN%E1%84%80%E1%85%B5%E1%84%8E%E1%85%AE%E1%86%AF100%E1%84%8C%E1%85%A6_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20473d340831814d60ba0766b87aa03f52/2-1.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_RNN%E1%84%80%E1%85%B5%E1%84%8E%E1%85%AE%E1%86%AF100%E1%84%8C%E1%85%A6_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20473d340831814d60ba0766b87aa03f52/2-1.png)

생각해보면 수학을 비롯한 다양한 학문들과 예술은 모두 Sequence라는 것을 알 수 있는데, 이는 인간의 생각이 마구잡이로 생성되는 것이 아닌, 어느 정도 흐름을 따라 이어지기 때문이다. 따라서 Sequence Model과 Sequence data를 잘 핸들링하는 것은 더 강력한 인공지능을 만드는 데 필수적이라고 보인다.

### 기억하는 모델의 등장: RNN

8장에서 우리는 Perceptron을 여러개 연결해서 다중 퍼셉트론을 만든 후 언어 모델로서 사용해보는 실습을 마쳤다. 하지만 여기에는 한 가지 한계점이 있는데, 그것은 다음 단어를 예측할 때 정해진 n개의 단어만을 참고한다는 것이다. 이렇게 되면 n개 단어 전에 있던 단어들은 버려지고 그 문맥을 사용할 수 없게 된다.

하지만 RNN의 경우 은닉층에서 나온 결과값이 출력층과 은닉층 방향으로 보내지면서 이전의 입력값이 후의 출력값에도 영향을 줄 수 있다.

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_RNN%E1%84%80%E1%85%B5%E1%84%8E%E1%85%AE%E1%86%AF100%E1%84%8C%E1%85%A6_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20473d340831814d60ba0766b87aa03f52/diags.jpeg](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_RNN%E1%84%80%E1%85%B5%E1%84%8E%E1%85%AE%E1%86%AF100%E1%84%8C%E1%85%A6_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20473d340831814d60ba0766b87aa03f52/diags.jpeg)

이 때, RNN의 입력과 출력 길이의 따라 각각 다른 용도로 사용될 수 있는데, 예를 들어 one-to-many모델은 하나의 이미지 입력에 대해서 image captioning하는 문제에 사용될 수 있고, many-to-many는 품사 태깅과 같은 문제에 사용된다.

은닉층이 여러 층인 Deep RNN이나 뒤 시점의 은닉 상태를 전달받아 현재의 은닉 상태를 계산하는 셀도 있는 Bidirectional RNN도 존재한다. 

### 장기 의존성 문제와 LSTM, GRU의 등장

장기 의존성 문제(The problem of Long-Term Dependencies)란 RNN의 time step이 길어질수록 앞의 시점의 정보가 충분히 전달되지 못하는 현상이다. 이를 해결하기 위해서 LSTM과 GRU가 나왔다.

(Q. 왜? 은닉층의 가중치를 크게 하면 괜찮지 않나?)

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_RNN%E1%84%80%E1%85%B5%E1%84%8E%E1%85%AE%E1%86%AF100%E1%84%8C%E1%85%A6_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20473d340831814d60ba0766b87aa03f52/vaniila_rnn_and_different_lstm_ver2.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_RNN%E1%84%80%E1%85%B5%E1%84%8E%E1%85%AE%E1%86%AF100%E1%84%8C%E1%85%A6_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20473d340831814d60ba0766b87aa03f52/vaniila_rnn_and_different_lstm_ver2.png)

LSTM은 큰 틀에서는 RNN과 유사하지만 은닉층을 계산할 때 좀 더 복잡한 과정을 거친다. 출력, 입력, 삭제 게이트라는 총 3개의 '게이트'가 존재하고 셀 상태라는 장기 기억을 위한 상태가 따로 존재한다.

GRU는 업데이트 게이트와 리셋 게이트 2개가 존재해 좀 더 구조가 간단하다고 할 수 있다. 구조가 간단한 만큼, 학습 속도 또한 빠르다고 알려져 있다. 일반적으로 데이터의 양이 적으면 매개 변수의 양이 적은 GRU가 더 낫고, 데이터의 양이 많으면 LSTM이 더 낫다고 알려져 있다.

### 실례: Image Captioning

Deep Visual-Semantic Alignments for Generating Image Descriptions - Andrej Karpathy, Li Fei-Fei (CVPR 2015)