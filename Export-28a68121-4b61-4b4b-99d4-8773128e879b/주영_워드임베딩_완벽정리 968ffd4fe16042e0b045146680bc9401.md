# 주영_워드임베딩_완벽정리

### 1. 워드 임베딩 (Word Embedding)

1. 희소 표현 (Sparse Representation): 단어의 인덱스 값만 1이고 나머지 인덱스는 전부 0으로 표현하는 방법. 원-핫 인코딩과 유사하다. 다만, 단어의 개수가 들어나게 되면 대부분이 0으로 표현되게 된다. 이를 희소 벡터(sparse vector)이라고 한다. 이 표현법의 문제점은 공간적 낭비를 일으킨다는 것과 단어의 의미 자체를 담지 못하는 것이다.
2. 밀집 표현 (Dense Representation): 벡터의 차원을 사용자가 설정한 값에 맞춘다. 10,000개의 단어가 있고 밀집 표현의 차원이 128이라면, 임의의 한 단어의 벡터의 모든 값이 실수가 된다.
3. 워드 임베딩 (Word Embedding): 단어를 밀집 벡터로 표현하는 방법을 말한다. 밀집 벡터는 임베딩 과정을 통해 나온 결과라고 말하기도 하기 때문에 이를 임베딩 벡터 (embedding  vector)이라고 한다. 임베딩의 방법론으로는 다음과 같은 방법이 있다.

    1) LSA

    2) Word2Vec

    3) Glove

    4) FastText

    LSA는 Chapter 6의 토픽 모델링 단원에서 다뤘었고 여기서는 Word2Vec, Glove와 같은 임베딩 방법론을 다룬다.

### 2. Word2Vec

원-핫 인코딩 또는 희소 표현 방법은 단어 간 유사도를 계산할 수 없는 단점이 있었다. 따라서 단어의 '의미'를 다차원 공간에 벡터화하는 방법을 찾게 되는데, 이를 분산 표현 (distributed representation)이라고 한다.

분산 표현은 다음과 같은 가정을 기본으로 한다. '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다.' 단어의 의미를 여러 차원에 분산하여 표현하기 때문에, 희소 벡터보다 더 적은 차원을 가진다. 예를 들어 10,000개의 단어가 있다면 그 안의 동의어의 의미를 여러 차원에 분산하여 표현하는 것이다.

분산 표현을 사용한 학습 방법으로는 NNLM, RNNLM 등이 있으나 Word2Vec이 위의 방법들의 속도를 대폭 개선하여 많이 사용되고 있다.

Word2Vec은 CBOW (Continuous Bag of Words)와 Skip-Gram의 두 가지 방식이 있다.

### 2.1 CBOW (Continuous Bag of Words)

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_%E1%84%8B%E1%85%AF%E1%84%83%E1%85%B3%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20968ffd4fe16042e0b045146680bc9401/1.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_%E1%84%8B%E1%85%AF%E1%84%83%E1%85%B3%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20968ffd4fe16042e0b045146680bc9401/1.png)

The fat cat sat on the mat이라는 문장에서 sat을 예측하는 문제를 생각해보자. 이 때, 예측하는 단어 sat을 중심 단어(center word)라고 하고 주변의 단어를 주변 단어(context word)라고 한다.

중심 단어를 예측하기 위해서 앞 뒤로 몇 개의 단어를 볼지를 결정해야하는데 이를 윈도우(window)라고 한다. 윈도우 크기가 n이라면, 참고하는 주변 단어의 개수는 2n이 된다.

윈도우의 크기를 정했다며 주변 단어와 중심 단어를 바꿔가며 데이터 셋을 만들 수 있는데 이를 슬라이딩 윈도우(sliding window)라고 한다.

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_%E1%84%8B%E1%85%AF%E1%84%83%E1%85%B3%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20968ffd4fe16042e0b045146680bc9401/2.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_%E1%84%8B%E1%85%AF%E1%84%83%E1%85%B3%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20968ffd4fe16042e0b045146680bc9401/2.png)

위의 사진은 CBOW의 인공 신경망을 도식화한 것이다. 두번째 층인 투사층(Projection Layer)은 일반적인 은닉층과 다른 층으로, 활성화 함수가 존재하지 않으며 룩업 테이블이라는 연산을 담당한다. 자세한 과정은 wikidocs의 나와있으므로 생략하겠다.

### 2.2 Skip-Gram

Skip-Gram은 CBOW와 유사한데, CBOW가 주변 단어를 통해 중심 단어를 예측했다면 skip-gram은 중심 단어를 통해 주변 단어를 예측하는 방법이다.

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_%E1%84%8B%E1%85%AF%E1%84%83%E1%85%B3%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20968ffd4fe16042e0b045146680bc9401/skipgram_dataset.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_%E1%84%8B%E1%85%AF%E1%84%83%E1%85%B3%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20968ffd4fe16042e0b045146680bc9401/skipgram_dataset.png)

![%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_%E1%84%8B%E1%85%AF%E1%84%83%E1%85%B3%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20968ffd4fe16042e0b045146680bc9401/word2vec_renew_6.png](%E1%84%8C%E1%85%AE%E1%84%8B%E1%85%A7%E1%86%BC_%E1%84%8B%E1%85%AF%E1%84%83%E1%85%B3%E1%84%8B%E1%85%B5%E1%86%B7%E1%84%87%E1%85%A6%E1%84%83%E1%85%B5%E1%86%BC_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%87%E1%85%A7%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20968ffd4fe16042e0b045146680bc9401/word2vec_renew_6.png)

여러 논문에서 성능을 비교한 결과 Skip-Gram이 CBOW보다 성능이 좀 더 나은 것으로 알려져 있다.

### 3. 글로브 (Glove)

글로브는 카운트 기반과 예측 기반을 모두 사용하는 방법론으로 카운트 기반의 LSA, 예측 기반의 Word2Vec 각각의 단점을 보완하며 나오게 되었다. 실제로는 Word2Vec만큼 뛰어난 성능을 보여준다.

### 3. 패스트텍스트 (FastText)

FastText는 Word2Vec 이후에 등장한 방법론으로, 기본적인 메카니즘은 Word2Vec의 화장이라고 볼 수 있다. 한 가지 차이점은, Word2Vec은 한 단어를 쪼개질 수 없는 단위로 생각하지만, FastText에서는 한 단어 안에도 여러 단어가 존재하는 것으로 간주한다. 이를 내부 단어(subword)를 고려한다고 한다. 단어는 글자 단위 n-gram의 구성으로 취급된다.

예를 들어, n=3일 때 apple이라는 단어의 경우,  app, ppl, ple로 분리되고 이들이 벡터가 된다. 또한 기존 단어에 <, 와 >를 붙인 토큰도 벡터화를 한다. 

단어를 내부 단어의 조합으로 생각했을 때의 장점은 모르는 단어에 대한 대응을 더 잘 할 수 있다는 점이다. 예를 들어, birth와 place라는 단어는 학습이 된 상태에서 birthplace라는 모르는 단어가 들어온다면, FastText는 내부 단어를 통해 birthplace의 벡터를 얻을 수 있게 된다. 이는 Word2Vec, Glove와 다른 점이다. 또한, Typo와 같은 노이즈가 많은 글에서 FastText는 강점을 가지는데, 이는 오타가 섞인 단어의 n-gram이 원래 단어의 n-gram과 동일하기 때문이다.

### 4. 엘모(Embeddings from Language Model, ELMo)